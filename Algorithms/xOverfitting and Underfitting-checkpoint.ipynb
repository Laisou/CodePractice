{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126caa68",
   "metadata": {},
   "source": [
    "code come from\n",
    "\n",
    "https://www.kaggle.com/code/laihaochan/exercise-overfitting-and-underfitting/edit\n",
    "\n",
    "how to improve training outcomes by including an early stopping callback to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-whitegrid') #old\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn API for styling\n",
    "sns.set(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('animation', html='html5')\n",
    "\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.deep_learning_intro.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88303402",
   "metadata": {},
   "source": [
    "First load the Spotify dataset. Your task will be to predict the popularity of a song based on various audio features, like 'tempo', 'danceability', and 'mode'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c36823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "spotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n",
    "\n",
    "X = spotify.copy().dropna()\n",
    "y = X.pop('track_popularity')\n",
    "artists = X['track_artist']\n",
    "\n",
    "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "features_cat = ['playlist_genre']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "# We'll do a \"grouped\" split to keep all of an artist's songs in one\n",
    "# split or the other. This is to help prevent signal leakage.\n",
    "def group_split(X, y, group, train_size=0.75):\n",
    "    splitter = GroupShuffleSplit(train_size=train_size)\n",
    "    train, test = next(splitter.split(X, y, groups=group))\n",
    "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = y_train / 100 # popularity is on a scale 0-100, so this rescales to 0-1.\n",
    "y_valid = y_valid / 100\n",
    "\n",
    "input_shape = [X_train.shape[1]]\n",
    "print(\"Input shape: {}\".format(input_shape))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24f538e7",
   "metadata": {},
   "source": [
    "Input shape: [18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d016300",
   "metadata": {},
   "source": [
    "Let's start with the simplest network, a linear model. This model has low capacity.\n",
    "\n",
    "Run this next cell without any changes to train a linear model on the Spotify dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1086a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(1, input_shape=input_shape),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=50,\n",
    "    verbose=0, # suppress output since we'll plot the curves\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63adc668",
   "metadata": {},
   "source": [
    "Minimum Validation Loss: 0.1946\n",
    "\n",
    "picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91624bca",
   "metadata": {},
   "source": [
    "It's not uncommon for the curves to follow a \"hockey stick\" pattern like you see here. This makes the final part of training hard to see, so let's start at epoch 10 instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ed3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the plot at epoch 10\n",
    "history_df.loc[10:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
   ]
  },
  {
   "cell_type": "raw",
   "id": "285febc1",
   "metadata": {},
   "source": [
    "Minimum Validation Loss: 0.1946\n",
    "\n",
    "picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa32a8",
   "metadata": {},
   "source": [
    "### 1.Evaluate Baseline\n",
    "\n",
    "What do you think? Would you say this model is underfitting, overfitting, just right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the solution (Run this cell to receive credit!)\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44721170",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "The gap between these curves is quite small and the validation loss never increases, so it's more likely that the network is underfitting than overfitting. It would be worth experimenting with more capacity to see if that's the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da37496",
   "metadata": {},
   "source": [
    "Now let's add some capacity to our network. We'll add three hidden layers with 128 units each. Run the next cell to train the network and see the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=50,\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce03c1a0",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "47/47 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.2034\n",
    "Epoch 2/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.2019 - val_loss: 0.1987\n",
    "Epoch 3/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 0.1966\n",
    "Epoch 4/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1937 - val_loss: 0.1976\n",
    "Epoch 5/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1913 - val_loss: 0.1957\n",
    "Epoch 6/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1898 - val_loss: 0.1969\n",
    "Epoch 7/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1889 - val_loss: 0.1961\n",
    "Epoch 8/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 0.1962\n",
    "Epoch 9/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 0.1979\n",
    "Epoch 10/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1862 - val_loss: 0.1954\n",
    "Epoch 11/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1850 - val_loss: 0.1968\n",
    "Epoch 12/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1841 - val_loss: 0.1963\n",
    "Epoch 13/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1836 - val_loss: 0.1950\n",
    "Epoch 14/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1827 - val_loss: 0.1952\n",
    "Epoch 15/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1819 - val_loss: 0.1963\n",
    "Epoch 16/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 0.1967\n",
    "Epoch 17/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 0.1985\n",
    "Epoch 18/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1804 - val_loss: 0.1962\n",
    "Epoch 19/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1798 - val_loss: 0.1961\n",
    "Epoch 20/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1793 - val_loss: 0.1957\n",
    "Epoch 21/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1784 - val_loss: 0.1974\n",
    "Epoch 22/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1780 - val_loss: 0.1973\n",
    "Epoch 23/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1782 - val_loss: 0.1960\n",
    "Epoch 24/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1773 - val_loss: 0.1967\n",
    "Epoch 25/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1766 - val_loss: 0.1974\n",
    "Epoch 26/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1764 - val_loss: 0.1996\n",
    "Epoch 27/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1760 - val_loss: 0.1988\n",
    "Epoch 28/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1750 - val_loss: 0.1998\n",
    "Epoch 29/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1749 - val_loss: 0.1995\n",
    "Epoch 30/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1743 - val_loss: 0.1992\n",
    "Epoch 31/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1734 - val_loss: 0.1980\n",
    "Epoch 32/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1732 - val_loss: 0.1992\n",
    "Epoch 33/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1727 - val_loss: 0.1985\n",
    "Epoch 34/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1721 - val_loss: 0.1994\n",
    "Epoch 35/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1714 - val_loss: 0.2001\n",
    "Epoch 36/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1711 - val_loss: 0.1993\n",
    "Epoch 37/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1706 - val_loss: 0.2011\n",
    "Epoch 38/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1713 - val_loss: 0.2011\n",
    "Epoch 39/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1703 - val_loss: 0.2000\n",
    "Epoch 40/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1693 - val_loss: 0.1995\n",
    "Epoch 41/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1685 - val_loss: 0.2012\n",
    "Epoch 42/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1681 - val_loss: 0.2006\n",
    "Epoch 43/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1682 - val_loss: 0.2007\n",
    "Epoch 44/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1678 - val_loss: 0.2012\n",
    "Epoch 45/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1676 - val_loss: 0.2032\n",
    "Epoch 46/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1675 - val_loss: 0.2013\n",
    "Epoch 47/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1666 - val_loss: 0.2034\n",
    "Epoch 48/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1658 - val_loss: 0.2021\n",
    "Epoch 49/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 0.2044\n",
    "Epoch 50/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1662 - val_loss: 0.2068\n",
    "Minimum Validation Loss: 0.1950\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee00d1e",
   "metadata": {},
   "source": [
    "### 2.Add Capacity\n",
    "\n",
    "What is your evaluation of these curves? Underfitting, overfitting, just right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88262b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the solution (Run this cell to receive credit!)\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "add08616",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "Now the validation loss begins to rise very early, while the training loss continues to decrease. This indicates that the network has begun to overfit. At this point, we would need to try something to prevent it, either by reducing the number of units or through a method like early stopping. (We'll see another in the next lesson!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ef502",
   "metadata": {},
   "source": [
    "### 3.Define Early Stopping Callback\n",
    "\n",
    "Now define an early stopping callback that waits 5 epochs (patience') for a change in validation loss of at least 0.001 (min_delta) and keeps the weights with the best loss (restore_best_weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# YOUR CODE HERE: define an early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Check your answer\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8eb4310",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_3.hint()\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95652ca6",
   "metadata": {},
   "source": [
    "Hint: Your solution should look something like:\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience=____,\n",
    "    min_delta=____,\n",
    "    restore_best_weights=____,\n",
    ")\n",
    "Solution:\n",
    "\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cb529",
   "metadata": {},
   "source": [
    "Now run this cell to train the model and get the learning curves. Notice the callbacks argument in model.fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(64, activation='relu'),    \n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b53075ee",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "47/47 [==============================] - 1s 8ms/step - loss: 0.2346 - val_loss: 0.2067\n",
    "Epoch 2/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.2026 - val_loss: 0.2009\n",
    "Epoch 3/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1971 - val_loss: 0.1979\n",
    "Epoch 4/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1939 - val_loss: 0.1981\n",
    "Epoch 5/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1923 - val_loss: 0.1991\n",
    "Epoch 6/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1915 - val_loss: 0.1972\n",
    "Epoch 7/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1893 - val_loss: 0.1970\n",
    "Epoch 8/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1880 - val_loss: 0.1969\n",
    "Epoch 9/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 0.1964\n",
    "Epoch 10/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 0.1964\n",
    "Epoch 11/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1857 - val_loss: 0.1964\n",
    "Epoch 12/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1848 - val_loss: 0.1953\n",
    "Epoch 13/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1837 - val_loss: 0.1970\n",
    "Epoch 14/50\n",
    "47/47 [==============================] - 0s 5ms/step - loss: 0.1835 - val_loss: 0.1966\n",
    "Epoch 15/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1830 - val_loss: 0.1998\n",
    "Epoch 16/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1825 - val_loss: 0.1974\n",
    "Epoch 17/50\n",
    "47/47 [==============================] - 0s 4ms/step - loss: 0.1818 - val_loss: 0.1970\n",
    "Minimum Validation Loss: 0.1953"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65911f59",
   "metadata": {},
   "source": [
    "### 4.Train and Interpret\n",
    "\n",
    "Was this an improvement compared to training without early stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the solution (Run this cell to receive credit!)\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "214200df",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "The early stopping callback did stop the training once the network began overfitting. Moreover, by including restore_best_weights we still get to keep the model where validation loss was lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82865a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
