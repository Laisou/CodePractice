{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba20fd7c",
   "metadata": {},
   "source": [
    "code come from\n",
    "\n",
    "https://www.kaggle.com/code/laihaochan/exercise-dropout-and-batch-normalization/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn API for styling\n",
    "sns.set(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('animation', html='html5')\n",
    "\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.deep_learning_intro.ex5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load the Spotify dataset.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "spotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n",
    "\n",
    "X = spotify.copy().dropna()\n",
    "y = X.pop('track_popularity')\n",
    "artists = X['track_artist']\n",
    "\n",
    "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "features_cat = ['playlist_genre']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "def group_split(X, y, group, train_size=0.75):\n",
    "    splitter = GroupShuffleSplit(train_size=train_size)\n",
    "    train, test = next(splitter.split(X, y, groups=group))\n",
    "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = y_train / 100\n",
    "y_valid = y_valid / 100\n",
    "\n",
    "input_shape = [X_train.shape[1]]\n",
    "print(\"Input shape: {}\".format(input_shape))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce61aa55",
   "metadata": {},
   "source": [
    "Input shape: [18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c371457",
   "metadata": {},
   "source": [
    "### Add Dropout to Spotify Model\n",
    "\n",
    "Add two dropout layers, one after the Dense layer with 128 units, and one after the Dense layer with 64 units. Set the dropout rate on both to 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add two 30% dropout layers, one after 128 and one after 64\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "020a51f9",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88023726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_1.hint()\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f537ceac",
   "metadata": {},
   "source": [
    "Hint: Your answer should look something like:\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Dense\n",
    "    # Dropout\n",
    "    # Dense\n",
    "    # Droput\n",
    "    # Dense\n",
    "])\n",
    "Solution:\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78918615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this next cell to train the model see the effect of adding dropout.\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=50,\n",
    "    verbose=0,\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e86006d3",
   "metadata": {},
   "source": [
    "Minimum Validation Loss: 0.1906"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979df83c",
   "metadata": {},
   "source": [
    "### Evaluate Dropout\n",
    "\n",
    "Recall from Exercise 4 that this model tended to overfit the data around epoch 5. Did adding dropout seem to help prevent overfitting this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the solution (Run this cell to receive credit!)\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf4e951e",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "From the learning curves, you can see that the validation loss remains near a constant minimum even though the training loss continues to decrease. So we can see that adding dropout did prevent overfitting this time. Moreover, by making it harder for the network to fit spurious patterns, dropout may have encouraged the network to seek out more of the true patterns, possibly improving the validation loss some as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f0bdc",
   "metadata": {},
   "source": [
    "Now, we'll switch topics to explore how batch normalization can fix problems in training.\n",
    "\n",
    "Load the Concrete dataset. We won't do any standardization this time. This will make the effect of batch normalization much more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed14c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concrete = pd.read_csv('../input/dl-course-data/concrete.csv')\n",
    "df = concrete.copy()\n",
    "\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_valid = df.drop(df_train.index)\n",
    "\n",
    "X_train = df_train.drop('CompressiveStrength', axis=1)\n",
    "X_valid = df_valid.drop('CompressiveStrength', axis=1)\n",
    "y_train = df_train['CompressiveStrength']\n",
    "y_valid = df_valid['CompressiveStrength']\n",
    "\n",
    "input_shape = [X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the following cell to train the network on the unstandardized Concrete data.\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='sgd', # SGD is more sensitive to differences of scale\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57cbe62e",
   "metadata": {},
   "source": [
    "Minimum Validation Loss: nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd7b94",
   "metadata": {},
   "source": [
    "Did you end up with a blank graph? Trying to train this network on this dataset will usually fail. Even when it does converge (due to a lucky weight initialization), it tends to converge to a very large number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeeb079",
   "metadata": {},
   "source": [
    "### Add Batch Normalization Layers\n",
    "\n",
    "Batch normalization can help correct problems like this.\n",
    "\n",
    "Add four BatchNormalization layers, one before each of the dense layers. (Remember to move the input_shape argument to the new first layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c28c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add a BatchNormalization layer before each Dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Check your answer\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b66f6f51",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_3.hint()\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5be53b1",
   "metadata": {},
   "source": [
    "Hint: Your answer should look something like:\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Batch Normalization\n",
    "    # Dense\n",
    "    # Batch Normalization\n",
    "    # Dense\n",
    "    # Batch Normalization\n",
    "    # Dense\n",
    "    # Batch Normalization\n",
    "    # Dense\n",
    "])\n",
    "Solution:\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the next cell to see if batch normalization will let us train the model.\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "EPOCHS = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0078bac1",
   "metadata": {},
   "source": [
    "Minimum Validation Loss: 4.0152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c53d0f",
   "metadata": {},
   "source": [
    "### Evaluate Batch Normalization\n",
    "\n",
    "Did adding batch normalization help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the solution (Run this cell to receive credit!)\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b3063eb",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "You can see that adding batch normalization was a big improvement on the first attempt! By adaptively scaling the data as it passes through the network, batch normalization can let you train models on difficult datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101b70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
