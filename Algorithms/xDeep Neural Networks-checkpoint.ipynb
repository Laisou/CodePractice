{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e88977b",
   "metadata": {},
   "source": [
    "code come from\n",
    "\n",
    "https://www.kaggle.com/code/laihaochan/exercise-deep-neural-networks/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7242",
   "metadata": {},
   "source": [
    "n the tutorial, we saw how to build deep neural networks by stacking layers inside a Sequential model. By adding an activation function after the hidden layers, we gave the network the ability to learn more complex (non-linear) relationships in the data.\n",
    "\n",
    "In these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn API for styling\n",
    "sns.set(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "# Setup feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.deep_learning_intro.ex2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d577c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset.\n",
    "import pandas as pd\n",
    "\n",
    "concrete = pd.read_csv('../input/dl-course-data/concrete.csv')\n",
    "concrete.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e868e3a",
   "metadata": {},
   "source": [
    "\tCement\tBlastFurnaceSlag\tFlyAsh\tWater\tSuperplasticizer\tCoarseAggregate\tFineAggregate\tAge\tCompressiveStrength\n",
    "0\t540.0\t0.0\t0.0\t162.0\t2.5\t1040.0\t676.0\t28\t79.99\n",
    "1\t540.0\t0.0\t0.0\t162.0\t2.5\t1055.0\t676.0\t28\t61.89\n",
    "2\t332.5\t142.5\t0.0\t228.0\t0.0\t932.0\t594.0\t270\t40.27\n",
    "3\t332.5\t142.5\t0.0\t228.0\t0.0\t932.0\t594.0\t365\t41.05\n",
    "4\t198.6\t132.4\t0.0\t192.0\t0.0\t978.4\t825.5\t360\t44.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a24644",
   "metadata": {},
   "source": [
    "### 1.Input Shape\n",
    "\n",
    "The target for this task is the column 'CompressiveStrength'. The remaining columns are the features we'll use as inputs.\n",
    "\n",
    "What would be the input shape for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "input_shape = [8]\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ca4837f",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a731cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_1.hint()\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c557887c",
   "metadata": {},
   "source": [
    "Hint: Remember to only count the input features when determining input_shape. You should not count the target (the CompressiveStrength column).\n",
    "\n",
    "Solution:\n",
    "\n",
    "\n",
    "input_shape = [8]\n",
    "# you could also use a 1-tuple, like input_shape = (8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75205283",
   "metadata": {},
   "source": [
    "### 3.Define a Model with Hidden Layers\n",
    "\n",
    "Now create a model with three hidden layers, each having 512 units and the ReLU activation. Be sure to include an output layer of one unit and no activation, and also input_shape as an argument to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f87e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),    \n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd9c49a8",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_2.hint()\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70cf1bc2",
   "metadata": {},
   "source": [
    "Hint: Your answer should look something like:\n",
    "\n",
    "model = keras.Sequential([\n",
    "    ____\n",
    "])\n",
    "Solution:\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),    \n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afd683",
   "metadata": {},
   "source": [
    "### 3.Activation Layers\n",
    "\n",
    "Let's explore activations functions some.\n",
    "\n",
    "The usual way of attaching an activation function to a Dense layer is to include it as part of the definition with the activation argument. Sometimes though you'll want to put some other layer between the Dense layer and its activation function. (We'll see an example of this in Lesson 5 with batch normalization.) In this case, we can define the activation in its own Activation layer, like so:\n",
    "\n",
    "    layers.Dense(units=8),\n",
    "    layers.Activation('relu')\n",
    "\n",
    "This is completely equivalent to the ordinary way: layers.Dense(units=8, activation='relu').\n",
    "\n",
    "Rewrite the following model so that each activation is in its own Activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c211c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE: rewrite this to use activation layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, input_shape=[8]),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(32),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Check your answer\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d183d",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines below will give you a hint or solution code\n",
    "q_3.hint()\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0bf999c",
   "metadata": {},
   "source": [
    "Hint: Your model should look something like:\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(____),\n",
    "    layers.Activation(____),\n",
    "    layers.Dense(____),\n",
    "    layers.Activation(____),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "Solution:\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, input_shape=[8]),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(32),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10796823",
   "metadata": {},
   "source": [
    "### Optional: Alternatives to ReLU\n",
    "\n",
    "There is a whole family of variants of the 'relu' activation -- 'elu', 'selu', and 'swish', among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with.\n",
    "\n",
    "Let's look at the graphs of some of these. Change the activation from 'relu' to one of the others named above. Then run the cell to see the graph. (Check out the documentation for more ideas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\n",
    "activation_layer = layers.Activation('relu')\n",
    "\n",
    "x = tf.linspace(-3.0, 3.0, 100)\n",
    "y = activation_layer(x) # once created, a layer is callable just like a function\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(x, y)\n",
    "plt.xlim(-3, 3)\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
